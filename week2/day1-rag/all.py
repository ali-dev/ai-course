# -*- coding: utf-8 -*-
"""Week2_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TrzIo9pith3nqO7tpiiJzo1yl5ulG7pZ
"""

# upload.py

from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_pinecone import PineconeVectorStore
from langchain_community.document_loaders.pdf import PyPDFLoader
from langchain_community.document_loaders import DirectoryLoader

# Prep documents to be uploaded to the vector database (Pinecone)
loader = DirectoryLoader('../', glob="**/*.pdf", loader_cls=PyPDFLoader)
raw_docs = loader.load()

# Split documents into smaller chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
documents = text_splitter.split_documents(raw_docs)
print(f"Going to add {len(documents)} to Pinecone")

# Choose the embedding model and vector store
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
PineconeVectorStore.from_documents(documents=documents, embedding=embeddings, index_name="bh-shareholder-letters")
print("Loading to vectorstore done")

# main.py

from langchain_openai import ChatOpenAI
from langchain.prompts.prompt import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain_pinecone import PineconeVectorStore
from langchain_openai import OpenAIEmbeddings
from constants import *

prompt = "How has Bershire Hathaway's investment in Coca-cola grown?"

# Note: we must use the same embedding model that we used when uploading the docs
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Querying the vector database for "relevant" docs
document_vectorstore = PineconeVectorStore(index_name="bh-shareholder-letters", embedding=embeddings)
retriever = document_vectorstore.as_retriever()
context = retriever.get_relevant_documents(prompt)
for doc in context:
    print(f"Source: {doc.metadata['source']}\nContent: {doc.page_content}\n\n")
print("__________________________")

# Adding context to our prompt
template = PromptTemplate(template="{query} Context: {context}", input_variables=["query", "context"])
prompt_with_context = template.invoke({"query": prompt, "context": context})

# Asking the LLM for a response from our prompt with the provided context
llm = ChatOpenAI(temperature=0.7)
results = llm.invoke(prompt_with_context)

print(results.content)

# constants.py

EMBEDDING_MODEL="text-embedding-3-small"
PINECONE_INDEX="bh-shareholder-letters"

# similarity_search.py

# pip install -U langchain-community faiss-cpu langchain-openai
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema.document import Document

# Initialize the embeddings class
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

# Embed a single query
query = "Hello, world!"
vector = embeddings.embed_query(query) # [281, 392, 34993, 23,...]
print(vector[:5])

# Embed multiple documents at once
documents = [
    Document(page_content="Alice works in finance"),
    Document(page_content="Bob is a database administrator"),
    Document(page_content="Carl manages Bob and Alice"),
]
vector_datastore = FAISS.from_documents(documents, embeddings)

# Perform a similarity search with scores
query = "Tell me about Alice"
docs_and_scores = vector_datastore.similarity_search_with_score(query)
print(docs_and_scores)